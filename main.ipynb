{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-18T08:08:47.378057Z",
     "iopub.status.busy": "2025-02-18T08:08:47.377596Z",
     "iopub.status.idle": "2025-02-18T08:09:07.854047Z",
     "shell.execute_reply": "2025-02-18T08:09:07.853541Z",
     "shell.execute_reply.started": "2025-02-18T08:08:47.378033Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in c:\\users\\ahmed\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from -r requirements.txt (line 1)) (3.5.0)\n",
      "Requirement already satisfied: trl in c:\\users\\ahmed\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from -r requirements.txt (line 2)) (0.18.1)\n",
      "Requirement already satisfied: peft in c:\\users\\ahmed\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from -r requirements.txt (line 3)) (0.15.2)\n",
      "Requirement already satisfied: bitsandbytes in c:\\users\\ahmed\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from -r requirements.txt (line 4)) (0.46.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\ahmed\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from datasets->-r requirements.txt (line 1)) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\ahmed\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from datasets->-r requirements.txt (line 1)) (2.2.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\ahmed\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from datasets->-r requirements.txt (line 1)) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\ahmed\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from datasets->-r requirements.txt (line 1)) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\ahmed\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from datasets->-r requirements.txt (line 1)) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\ahmed\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from datasets->-r requirements.txt (line 1)) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\ahmed\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from datasets->-r requirements.txt (line 1)) (4.67.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\ahmed\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from datasets->-r requirements.txt (line 1)) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\ahmed\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from datasets->-r requirements.txt (line 1)) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in c:\\users\\ahmed\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets->-r requirements.txt (line 1)) (2024.12.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\ahmed\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from datasets->-r requirements.txt (line 1)) (3.11.18)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in c:\\users\\ahmed\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from datasets->-r requirements.txt (line 1)) (0.30.2)\n",
      "Requirement already satisfied: packaging in c:\\users\\ahmed\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from datasets->-r requirements.txt (line 1)) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\ahmed\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from datasets->-r requirements.txt (line 1)) (6.0.2)\n",
      "Requirement already satisfied: accelerate>=0.34.0 in c:\\users\\ahmed\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from trl->-r requirements.txt (line 2)) (1.7.0)\n",
      "Requirement already satisfied: transformers>=4.50.0 in c:\\users\\ahmed\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from trl->-r requirements.txt (line 2)) (4.51.3)\n",
      "Requirement already satisfied: psutil in c:\\users\\ahmed\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from peft->-r requirements.txt (line 3)) (7.0.0)\n",
      "Requirement already satisfied: torch>=1.13.0 in c:\\users\\ahmed\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from peft->-r requirements.txt (line 3)) (2.7.0+cpu)\n",
      "Requirement already satisfied: safetensors in c:\\users\\ahmed\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from peft->-r requirements.txt (line 3)) (0.5.3)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\ahmed\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from torch>=1.13.0->peft->-r requirements.txt (line 3)) (4.13.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\ahmed\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from torch>=1.13.0->peft->-r requirements.txt (line 3)) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\ahmed\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from torch>=1.13.0->peft->-r requirements.txt (line 3)) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\ahmed\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from torch>=1.13.0->peft->-r requirements.txt (line 3)) (3.1.6)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\ahmed\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\ahmed\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\ahmed\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\ahmed\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\ahmed\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\ahmed\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\ahmed\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (1.20.0)\n",
      "Requirement already satisfied: idna>=2.0 in c:\\users\\ahmed\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from yarl<2.0,>=1.17.0->aiohttp->datasets->-r requirements.txt (line 1)) (3.10)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ahmed\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from requests>=2.32.2->datasets->-r requirements.txt (line 1)) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ahmed\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from requests>=2.32.2->datasets->-r requirements.txt (line 1)) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ahmed\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from requests>=2.32.2->datasets->-r requirements.txt (line 1)) (2025.1.31)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\ahmed\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from sympy>=1.13.3->torch>=1.13.0->peft->-r requirements.txt (line 3)) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\ahmed\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from tqdm>=4.66.3->datasets->-r requirements.txt (line 1)) (0.4.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\ahmed\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from transformers>=4.50.0->trl->-r requirements.txt (line 2)) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\ahmed\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from transformers>=4.50.0->trl->-r requirements.txt (line 2)) (0.21.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\ahmed\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from jinja2->torch>=1.13.0->peft->-r requirements.txt (line 3)) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\ahmed\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from pandas->datasets->-r requirements.txt (line 1)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\ahmed\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from pandas->datasets->-r requirements.txt (line 1)) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\ahmed\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from pandas->datasets->-r requirements.txt (line 1)) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ahmed\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets->-r requirements.txt (line 1)) (1.17.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bitsandbytes in c:\\users\\ahmed\\anaconda3\\envs\\machinelearning\\lib\\site-packages (0.46.0)\n",
      "Requirement already satisfied: torch<3,>=2.2 in c:\\users\\ahmed\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from bitsandbytes) (2.7.0+cpu)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\ahmed\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from bitsandbytes) (2.2.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\ahmed\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from torch<3,>=2.2->bitsandbytes) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\ahmed\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from torch<3,>=2.2->bitsandbytes) (4.13.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\ahmed\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from torch<3,>=2.2->bitsandbytes) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\ahmed\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from torch<3,>=2.2->bitsandbytes) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\ahmed\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from torch<3,>=2.2->bitsandbytes) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\ahmed\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from torch<3,>=2.2->bitsandbytes) (2024.12.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\ahmed\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from sympy>=1.13.3->torch<3,>=2.2->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\ahmed\\anaconda3\\envs\\machinelearning\\lib\\site-packages (from jinja2->torch<3,>=2.2->bitsandbytes) (3.0.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt\n",
    "!pip install --upgrade bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-02-18T08:09:07.855191Z",
     "iopub.status.busy": "2025-02-18T08:09:07.854983Z",
     "iopub.status.idle": "2025-02-18T08:09:08.149020Z",
     "shell.execute_reply": "2025-02-18T08:09:08.148460Z",
     "shell.execute_reply.started": "2025-02-18T08:09:07.855174Z"
    },
    "id": "W2yGmrJ9LZgl",
    "outputId": "c577413f-968b-4af1-c918-2793b1ad9dc5"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "access_token = \"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\"\n",
    "login(token=access_token)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wUDTL3n8MLLv"
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-18T08:09:08.150048Z",
     "iopub.status.busy": "2025-02-18T08:09:08.149849Z",
     "iopub.status.idle": "2025-02-18T08:09:15.792339Z",
     "shell.execute_reply": "2025-02-18T08:09:15.791825Z",
     "shell.execute_reply.started": "2025-02-18T08:09:08.150033Z"
    },
    "id": "ooBAfXQxL00m"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from datasets.arrow_dataset import Dataset\n",
    "\n",
    "def format_sample(sample):\n",
    "    \"\"\" Helper function to format a single input sample\"\"\"\n",
    "    instruction=sample['instruction']\n",
    "    input_text=sample['input']\n",
    "    output_text=sample['output']\n",
    "\n",
    "    if input_text is None or input_text==\"\":\n",
    "        formatted_prompt=(\n",
    "            f\"<|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
    "            f\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n\"\n",
    "            f\"### Instruction:\\n{instruction}\\n\\n\"\n",
    "            f\"### Response:\\n<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "            f\"{output_text}<|eot_id|>\"\n",
    "        )\n",
    "    else:\n",
    "        formatted_prompt=(\n",
    "            f\"<|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
    "            f\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n\"\n",
    "            f\"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input_text}\\n\\n\"\n",
    "            f\"### Response:\\n<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "            f\"{output_text}<|eot_id|>\"\n",
    "        )\n",
    "    formatted_prompt=\"\".join(formatted_prompt) # exclude trailing white spaces\n",
    "    return formatted_prompt                    # stream text into the dataloader, one by one\n",
    "\n",
    "\n",
    "\n",
    "def gen_train_input():\n",
    "    \"\"\" Format all data input in alpaca style\n",
    "        Return:\n",
    "            A generator on train data \"train_gen\"\n",
    "    \"\"\"\n",
    "    # load data\n",
    "    ds=load_dataset(\"iamtarun/python_code_instructions_18k_alpaca\",streaming=True, split=\"train\")\n",
    "    # datata set has 18.6k samples, we use 16.8k (90%) for training + 1.8k for validation\n",
    "    num_samples=16800\n",
    "    counter=0\n",
    "    for sample in iter(ds):\n",
    "        if counter>=num_samples:\n",
    "            break\n",
    "        formatted_prompt=format_sample(sample)\n",
    "        yield {'text': formatted_prompt}\n",
    "        counter+=1\n",
    "\n",
    "\n",
    "def gen_val_input():\n",
    "    \"\"\" Format all data input in alpaca style\n",
    "        Return:\n",
    "            A generator on val data \"val_gen\"\n",
    "    \"\"\"\n",
    "    # load data\n",
    "    ds=load_dataset(\"iamtarun/python_code_instructions_18k_alpaca\",streaming=True, split=\"train\")\n",
    "    # datata set has 18.6k samples, we use 16.8k (90%) for training + 1.8k for validation\n",
    "    num_samples=16800\n",
    "    counter=0\n",
    "    for sample in iter(ds):\n",
    "        if counter<num_samples:\n",
    "            counter+=1\n",
    "            continue\n",
    "\n",
    "        formatted_prompt=format_sample(sample)\n",
    "        yield {'text': formatted_prompt}\n",
    "        counter+=1\n",
    "\n",
    "dataset_train = Dataset.from_generator(gen_train_input)\n",
    "dataset_val=Dataset.from_generator(gen_val_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-02-18T08:12:57.243357Z",
     "iopub.status.busy": "2025-02-18T08:12:57.242929Z",
     "iopub.status.idle": "2025-02-18T08:12:57.247997Z",
     "shell.execute_reply": "2025-02-18T08:12:57.247490Z",
     "shell.execute_reply.started": "2025-02-18T08:12:57.243338Z"
    },
    "id": "GwPH7uvML02s",
    "outputId": "2dd352d8-a949-41f8-bb76-68196b253c59"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 16800\n",
      "Validation dataset size: 1812\n",
      "Sample train:\n",
      "{'text': '<|start_header_id|>user<|end_header_id|>\\n\\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nCreate a function to calculate the sum of a sequence of integers.\\n\\n### Input:\\n[1, 2, 3, 4, 5]\\n\\n### Response:\\n<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n# Python code\\ndef sum_sequence(sequence):\\n  sum = 0\\n  for num in sequence:\\n    sum += num\\n  return sum<|eot_id|>'}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train dataset size: {len(dataset_train)}\")\n",
    "print(f\"Validation dataset size: {len(dataset_val)}\")\n",
    "\n",
    "print(f\"Sample train:\\n{dataset_train[0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hyqrYyOvSYFO"
   },
   "source": [
    "## Model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-18T08:19:22.919097Z",
     "iopub.status.busy": "2025-02-18T08:19:22.918650Z",
     "iopub.status.idle": "2025-02-18T08:19:25.357067Z",
     "shell.execute_reply": "2025-02-18T08:19:25.356436Z",
     "shell.execute_reply.started": "2025-02-18T08:19:22.919078Z"
    },
    "id": "6ZIveBRhL04g"
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "The paging file is too small for this operation to complete. (os error 1455)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 37\u001b[39m\n\u001b[32m     33\u001b[39m     tokenizer.padding_side = \u001b[33m\"\u001b[39m\u001b[33mright\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     35\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m model,peft_config,tokenizer\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m model,peft_config,tokenizer=\u001b[43mcreate_and_prepare_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36mcreate_and_prepare_model\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate_and_prepare_model\u001b[39m():\n\u001b[32m      8\u001b[39m     bnb_config = BitsAndBytesConfig(\n\u001b[32m      9\u001b[39m         load_in_4bit=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     10\u001b[39m         bnb_4bit_quant_type=\u001b[33m\"\u001b[39m\u001b[33mnf4\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     11\u001b[39m         bnb_4bit_compute_dtype=torch.bfloat16,\n\u001b[32m     12\u001b[39m         bnb_4bit_use_double_quant=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     13\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m     model = \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcpu\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Run on CPU\u001b[39;49;00m\n\u001b[32m     18\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat32\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Full precision\u001b[39;49;00m\n\u001b[32m     19\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43maccess_token\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m     peft_config=LoraConfig(\n\u001b[32m     23\u001b[39m         lora_alpha=\u001b[32m16\u001b[39m,\n\u001b[32m     24\u001b[39m         r=\u001b[32m8\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     28\u001b[39m         target_modules=[\u001b[33m'\u001b[39m\u001b[33mq_proj\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mk_proj\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mv_proj\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m     29\u001b[39m     )\n\u001b[32m     31\u001b[39m     tokenizer=AutoTokenizer.from_pretrained(model_name,token=access_token)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ahmed\\anaconda3\\envs\\MachineLearning\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:571\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    569\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m model_class.config_class == config.sub_configs.get(\u001b[33m\"\u001b[39m\u001b[33mtext_config\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    570\u001b[39m         config = config.get_text_config()\n\u001b[32m--> \u001b[39m\u001b[32m571\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    572\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    573\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    574\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    575\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.\u001b[34m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    576\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(c.\u001b[34m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m._model_mapping.keys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    577\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ahmed\\anaconda3\\envs\\MachineLearning\\Lib\\site-packages\\transformers\\modeling_utils.py:279\u001b[39m, in \u001b[36mrestore_default_torch_dtype.<locals>._wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    277\u001b[39m old_dtype = torch.get_default_dtype()\n\u001b[32m    278\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m279\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    280\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    281\u001b[39m     torch.set_default_dtype(old_dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ahmed\\anaconda3\\envs\\MachineLearning\\Lib\\site-packages\\transformers\\modeling_utils.py:4399\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   4389\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   4390\u001b[39m         torch.set_default_dtype(dtype_orig)\n\u001b[32m   4392\u001b[39m     (\n\u001b[32m   4393\u001b[39m         model,\n\u001b[32m   4394\u001b[39m         missing_keys,\n\u001b[32m   4395\u001b[39m         unexpected_keys,\n\u001b[32m   4396\u001b[39m         mismatched_keys,\n\u001b[32m   4397\u001b[39m         offload_index,\n\u001b[32m   4398\u001b[39m         error_msgs,\n\u001b[32m-> \u001b[39m\u001b[32m4399\u001b[39m     ) = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4400\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4401\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4402\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcheckpoint_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4403\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4404\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4405\u001b[39m \u001b[43m        \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4406\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4407\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4408\u001b[39m \u001b[43m        \u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4409\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4410\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4411\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4412\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4413\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkey_mapping\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkey_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4414\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4415\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4417\u001b[39m \u001b[38;5;66;03m# make sure token embedding weights are still tied if needed\u001b[39;00m\n\u001b[32m   4418\u001b[39m model.tie_weights()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ahmed\\anaconda3\\envs\\MachineLearning\\Lib\\site-packages\\transformers\\modeling_utils.py:4638\u001b[39m, in \u001b[36mPreTrainedModel._load_pretrained_model\u001b[39m\u001b[34m(cls, model, state_dict, checkpoint_files, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, device_map, disk_offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_regex, device_mesh, key_mapping, weights_only)\u001b[39m\n\u001b[32m   4635\u001b[39m     original_checkpoint_keys = \u001b[38;5;28mlist\u001b[39m(state_dict.keys())\n\u001b[32m   4636\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   4637\u001b[39m     original_checkpoint_keys = \u001b[38;5;28mlist\u001b[39m(\n\u001b[32m-> \u001b[39m\u001b[32m4638\u001b[39m         \u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_files\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmeta\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m)\u001b[49m.keys()\n\u001b[32m   4639\u001b[39m     )\n\u001b[32m   4641\u001b[39m \u001b[38;5;66;03m# Check if we are in a special state, i.e. loading from a state dict coming from a different architecture\u001b[39;00m\n\u001b[32m   4642\u001b[39m prefix = model.base_model_prefix\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ahmed\\anaconda3\\envs\\MachineLearning\\Lib\\site-packages\\transformers\\modeling_utils.py:513\u001b[39m, in \u001b[36mload_state_dict\u001b[39m\u001b[34m(checkpoint_file, is_quantized, map_location, weights_only)\u001b[39m\n\u001b[32m    509\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    510\u001b[39m \u001b[33;03mReads a `safetensor` or a `.bin` checkpoint file. We load the checkpoint on \"cpu\" by default.\u001b[39;00m\n\u001b[32m    511\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    512\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m checkpoint_file.endswith(\u001b[33m\"\u001b[39m\u001b[33m.safetensors\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m is_safetensors_available():\n\u001b[32m--> \u001b[39m\u001b[32m513\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43msafe_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframework\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m    514\u001b[39m         metadata = f.metadata()\n\u001b[32m    516\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m metadata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m metadata.get(\u001b[33m\"\u001b[39m\u001b[33mformat\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mtf\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mflax\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmlx\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[31mOSError\u001b[39m: The paging file is too small for this operation to complete. (os error 1455)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from peft import LoraConfig, AutoPeftModelForCausalLM\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, BitsAndBytesConfig\n",
    "from trl import SFTTrainer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"unsloth/Llama-3.2-1B-Instruct\")\n",
    "model_name = AutoModelForCausalLM.from_pretrained(\"unsloth/Llama-3.2-1B-Instruct\")\n",
    "\n",
    "def create_and_prepare_model():\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "    )\n",
    "\n",
    "    model=AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        token=access_token\n",
    "    )\n",
    "\n",
    "    peft_config=LoraConfig(\n",
    "        lora_alpha=16,\n",
    "        r=8,\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        target_modules=['q_proj', 'k_proj', 'v_proj'],\n",
    "    )\n",
    "    \n",
    "    tokenizer=AutoTokenizer.from_pretrained(model_name,token=access_token)\n",
    "    tokenizer.pad_token = \"<|finetune_right_pad_id|>\"\n",
    "    tokenizer.padding_side = \"right\"\n",
    "\n",
    "    return model,peft_config,tokenizer\n",
    "\n",
    "model,peft_config,tokenizer=create_and_prepare_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "No CUDA GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No CUDA GPU\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4-q42NQzSbK_"
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-18T08:19:31.464023Z",
     "iopub.status.busy": "2025-02-18T08:19:31.463410Z",
     "iopub.status.idle": "2025-02-18T09:36:42.257159Z",
     "shell.execute_reply": "2025-02-18T09:36:42.256689Z",
     "shell.execute_reply.started": "2025-02-18T08:19:31.463996Z"
    },
    "id": "Vma9uK6yaXLS"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_87/3941464550.py:27: FutureWarning: `tokenizer` is deprecated and removed starting from version 0.16.0 for `SFTTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer=SFTTrainer(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bd65a90bdc747a999dd5a43a2b3fa26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template to train dataset:   0%|          | 0/16800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48fc41c06bcf446fbe369fbdea461090",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/16800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be1b5f7918d24cf19fa143b4e0747d6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/16800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "179d7896ea914017a0990fc2ca14932f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template to eval dataset:   0%|          | 0/1812 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85d29f1b88df4787bfa05b01f8cd7002",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval dataset:   0%|          | 0/1812 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "473ff8434df949f6a0e8feecb0d30e60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval dataset:   0%|          | 0/1812 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='393' max='393' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [393/393 1:16:42, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.290900</td>\n",
       "      <td>0.875727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.870100</td>\n",
       "      <td>0.839814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.845300</td>\n",
       "      <td>0.829108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.837300</td>\n",
       "      <td>0.823679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.834600</td>\n",
       "      <td>0.819919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.848100</td>\n",
       "      <td>0.818405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.823700</td>\n",
       "      <td>0.817808</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=393, training_loss=0.8984256191108063, metrics={'train_runtime': 4617.4541, 'train_samples_per_second': 10.915, 'train_steps_per_second': 0.085, 'total_flos': 1.5350980937613312e+17, 'train_loss': 0.8984256191108063})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from trl import SFTConfig, SFTTrainer\n",
    "\n",
    "args=SFTConfig(\n",
    "    output_dir=\"./llama32-python\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    gradient_accumulation_steps=8,\n",
    "    gradient_checkpointing=True, # to save memmory\n",
    "    optim=\"adamw_torch_fused\",\n",
    "    logging_steps=50,            # log train loss\n",
    "    # Add evaluation strategy to compute validation loss during training\n",
    "    evaluation_strategy=\"steps\",  # Evaluate at the end of each epoch\n",
    "    eval_steps=50,  # Evaluate every 50 steps    \n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-4,\n",
    "    bf16=True, \n",
    "    tf32=True, # enable true for faster speed (supported in higher-end gpu)\n",
    "    max_grad_norm=0.3,\n",
    "    warmup_ratio=0.03, # follow QLoRA paper\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    report_to=\"tensorboard\",\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "    dataset_text_field=\"text\",\n",
    ")\n",
    "\n",
    "trainer=SFTTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=dataset_train,\n",
    "    eval_dataset=dataset_val,\n",
    "    peft_config=peft_config,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-18T09:41:48.341018Z",
     "iopub.status.busy": "2025-02-18T09:41:48.340517Z",
     "iopub.status.idle": "2025-02-18T09:41:48.364800Z",
     "shell.execute_reply": "2025-02-18T09:41:48.364217Z",
     "shell.execute_reply.started": "2025-02-18T09:41:48.341000Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# save model state_dict\u001b[39;00m\n\u001b[1;32m      2\u001b[0m model_file_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLLAMA32_ft_python_code.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(\u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mstate_dict(), model_file_name)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel saved as \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_file_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# free the memory \u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# save model state_dict\n",
    "model_file_name=\"LLAMA32_ft_python_code.pth\"\n",
    "torch.save(model.state_dict(), model_file_name)\n",
    "print(f\"Model saved as {model_file_name}\")\n",
    "\n",
    "# free the memory \n",
    "del model\n",
    "del trainer\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load finetune model and run inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-18T09:45:29.269012Z",
     "iopub.status.busy": "2025-02-18T09:45:29.268551Z",
     "iopub.status.idle": "2025-02-18T09:45:35.149545Z",
     "shell.execute_reply": "2025-02-18T09:45:35.149112Z",
     "shell.execute_reply.started": "2025-02-18T09:45:29.268993Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading fine-tuned model ...\n",
      "Total parameters: 1,236,994,048\n",
      "Trainable parameters: 1,179,648\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel, LoraConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "def load_fine_tune_model(base_model_id, saved_weights):\n",
    "    # Load tokenizer and base model\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model_id)\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(base_model_id)\n",
    "    base_model.to(device)\n",
    "    \n",
    "    # Create LoRA config - make sure these parameters match your training configuration\n",
    "    peft_config = LoraConfig(\n",
    "        lora_alpha=16,\n",
    "        r=8,\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        target_modules=['q_proj','k_proj','v_proj'],\n",
    "    )\n",
    "    \n",
    "    # Initialize PeftModel\n",
    "    lora_model = PeftModel(base_model, peft_config)\n",
    "    \n",
    "    # Load the saved weights\n",
    "    state_dict = torch.load(saved_weights,map_location=device)\n",
    "        \n",
    "    # Create new state dict with correct prefixes and structure\n",
    "    new_state_dict = {}\n",
    "    for key, value in state_dict.items():\n",
    "        # key start with \"model\" -> add \"base_\" to the new key for base_model\n",
    "        new_key = f\"base_{key}\"        \n",
    "        new_state_dict[new_key] = value\n",
    "    \n",
    "    # Load the weights with strict=False to allow partial loading\n",
    "    lora_model.load_state_dict(new_state_dict, strict=False)\n",
    "    \n",
    "    # Set to evaluation mode\n",
    "    lora_model = lora_model.eval()\n",
    "    \n",
    "    return lora_model, tokenizer\n",
    "\n",
    "# Original model and saved weight\n",
    "base_model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "lora_weights = \"LLAMA32_ft_python_code.pth\"\n",
    "\n",
    "# Load model\n",
    "print(\"Loading fine-tuned model ...\")\n",
    "model_ft, tokenizer = load_fine_tune_model(base_model_id, lora_weights)\n",
    "total_params=sum(p.numel() for p in model_ft.parameters())\n",
    "trainable_params=sum(p.numel() for p in model_ft.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-18T09:52:30.282414Z",
     "iopub.status.busy": "2025-02-18T09:52:30.282183Z",
     "iopub.status.idle": "2025-02-18T09:52:30.288577Z",
     "shell.execute_reply": "2025-02-18T09:52:30.287942Z",
     "shell.execute_reply.started": "2025-02-18T09:52:30.282398Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate(model, prompt, tokenizer, max_new_tokens, context_size=512, temperature=0.0, top_k=1, eos_id=[128001, 128009]):\n",
    "    \"\"\"\n",
    "    Generate text using a language model with proper dtype handling and improved sampling.\n",
    "    \"\"\"\n",
    "    # Get model's expected dtype\n",
    "    model_dtype = next(model.parameters()).dtype\n",
    "    model_device = next(model.parameters()).device\n",
    "    \n",
    "    formatted_prompt = (\n",
    "        f\"<|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
    "        f\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n\"\n",
    "        f\"### Instruction:\\n{prompt}\"\n",
    "        f\"### Response:\\n<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "    )\n",
    "    \n",
    "    # Encode and prepare input\n",
    "    idx = tokenizer.encode(formatted_prompt)\n",
    "    idx = torch.tensor(idx, dtype=torch.long, device=model_device).unsqueeze(0)\n",
    "    num_tokens = idx.shape[1]\n",
    "    \n",
    "    # Generation loop\n",
    "    for _ in range(max_new_tokens):\n",
    "        # Get conditioning context\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        \n",
    "        # Generate logits\n",
    "        with torch.no_grad():            \n",
    "            # Forward pass - get output directly from model\n",
    "            outputs = model(\n",
    "                input_ids=idx_cond,\n",
    "                use_cache=False\n",
    "            )\n",
    "            \n",
    "            # Get logits directly from the output\n",
    "            logits = outputs.logits\n",
    "        \n",
    "        # Focus on last time step\n",
    "        logits = logits[:, -1, :]\n",
    "        \n",
    "        # Apply top-k filtering\n",
    "        if top_k is not None and top_k > 0:\n",
    "            top_logits, _ = torch.topk(logits, top_k)\n",
    "            min_val = top_logits[:, [-1]]\n",
    "            logits = torch.where(\n",
    "                logits < min_val,\n",
    "                torch.tensor(float('-inf'), device=model_device, dtype=model_dtype),\n",
    "                logits\n",
    "            )\n",
    "        \n",
    "        # Apply temperature and sample\n",
    "        if temperature > 0.0:\n",
    "            logits = logits / temperature\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "        else:\n",
    "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "        \n",
    "        # Check for EOS\n",
    "        if idx_next.item() in eos_id:\n",
    "            break\n",
    "            \n",
    "        # Append new token\n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "    \n",
    "    # Decode generated text\n",
    "    generated_ids = idx.squeeze(0)[num_tokens:]\n",
    "    generated_text = tokenizer.decode(generated_ids)\n",
    "    \n",
    "    return generated_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-18T09:52:46.711291Z",
     "iopub.status.busy": "2025-02-18T09:52:46.710849Z",
     "iopub.status.idle": "2025-02-18T09:52:49.836817Z",
     "shell.execute_reply": "2025-02-18T09:52:49.835986Z",
     "shell.execute_reply.started": "2025-02-18T09:52:46.711272Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```python\n",
      "def fibonacci(n):\n",
      "    \"\"\"\n",
      "    This function computes the nth Fibonacci number.\n",
      "\n",
      "    Args:\n",
      "        n (int): The position of the Fibonacci number to be computed.\n",
      "\n",
      "    Returns:\n",
      "        int: The nth Fibonacci number.\n",
      "    \"\"\"\n",
      "\n",
      "    # Base cases for the Fibonacci sequence\n",
      "    if n <= 0:\n",
      "        return \"Input should be a positive integer.\"\n",
      "    elif n == 1:\n",
      "        return 0\n",
      "    elif n == 2:\n",
      "        return 1\n",
      "\n",
      "    # Initialize the first two Fibonacci numbers\n",
      "    a, b = 0, 1\n",
      "\n",
      "    # Compute the nth Fibonacci number\n",
      "    for _ in range(2, n):\n",
      "        # Update a and b to be the sum of the previous two\n",
      "        a, b = b, a + b\n",
      "\n",
      "    # Return the nth Fibonacci number\n",
      "    return b\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "prompt = (\"Write a function that computes fibonacci numbers.\")\n",
    "print(generate(model_ft, prompt, tokenizer, max_new_tokens=512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-18T09:53:56.685333Z",
     "iopub.status.busy": "2025-02-18T09:53:56.684829Z",
     "iopub.status.idle": "2025-02-18T09:54:05.269256Z",
     "shell.execute_reply": "2025-02-18T09:54:05.268650Z",
     "shell.execute_reply.started": "2025-02-18T09:53:56.685314Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Solution\n",
      "\n",
      "```python\n",
      "def findNumbers(nums):\n",
      "    \"\"\"\n",
      "    This function takes an array of integers as input and returns the count of numbers \n",
      "    that contain an even number of digits.\n",
      "\n",
      "    Args:\n",
      "        nums (list): A list of integers.\n",
      "\n",
      "    Returns:\n",
      "        int: The count of numbers with an even number of digits.\n",
      "    \"\"\"\n",
      "    count = 0  # Initialize a counter variable to store the count of numbers with even digits\n",
      "    for num in nums:  # Iterate over each number in the input list\n",
      "        digits = len(str(abs(num)))  # Convert the number to a string and count the number of digits\n",
      "        if digits % 2 == 0:  # Check if the number of digits is even\n",
      "            count += 1  # If the number of digits is even, increment the counter\n",
      "    return count  # Return the total count of numbers with an even number of digits\n",
      "```\n",
      "\n",
      "### Example Use Cases\n",
      "\n",
      "```python\n",
      "# Test the function with an array of numbers\n",
      "print(findNumbers([1, 2, 3, 4, 5]))  # Output: 2\n",
      "\n",
      "# Test the function with an array of numbers with an even number of digits\n",
      "print(findNumbers([10, 20, 30, 40, 50]))  # Output: 5\n",
      "\n",
      "# Test the function with an array of numbers with an odd number of digits\n",
      "print(findNumbers([100, 200, 300, 400, 500]))  # Output: 0\n",
      "```\n",
      "\n",
      "This solution works by iterating over each number in the input list, converting it to a string to count the number of digits, and checking if the number of digits is even. If it is, the counter is incremented. Finally, the total count of numbers with an even number of digits is returned.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Given an array nums of integers, return how many of them contain an even number of digits.\"\n",
    "print(generate(model_ft, prompt, tokenizer, max_new_tokens=512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "MachineLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
